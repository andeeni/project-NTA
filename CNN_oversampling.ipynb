{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN oversampling.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYpDsiD9-t4T"
      },
      "source": [
        "## Load the dataset and create the vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ihvCKQAs-Hcm",
        "outputId": "2c8a6817-2b05-4d9d-8cd5-2111f5d404a3"
      },
      "source": [
        "!pip install dvc"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting dvc\n",
            "  Downloading dvc-2.8.3-py3-none-any.whl (399 kB)\n",
            "\u001b[K     |████████████████████████████████| 399 kB 13.0 MB/s \n",
            "\u001b[?25hCollecting python-benedict>=0.24.2\n",
            "  Downloading python_benedict-0.24.3-py3-none-any.whl (41 kB)\n",
            "\u001b[K     |████████████████████████████████| 41 kB 39 kB/s \n",
            "\u001b[?25hCollecting voluptuous>=0.11.7\n",
            "  Downloading voluptuous-0.12.2.tar.gz (48 kB)\n",
            "\u001b[K     |████████████████████████████████| 48 kB 4.9 MB/s \n",
            "\u001b[?25hCollecting dpath<3,>=2.0.2\n",
            "  Downloading dpath-2.0.5-py3-none-any.whl (15 kB)\n",
            "Collecting rich>=10.13.0\n",
            "  Downloading rich-10.15.2-py3-none-any.whl (214 kB)\n",
            "\u001b[K     |████████████████████████████████| 214 kB 43.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: toml>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from dvc) (0.10.2)\n",
            "Requirement already satisfied: packaging>=19.0 in /usr/local/lib/python3.7/dist-packages (from dvc) (21.3)\n",
            "Collecting distro>=1.3.0\n",
            "  Downloading distro-1.6.0-py2.py3-none-any.whl (19 kB)\n",
            "Collecting dulwich>=0.20.23\n",
            "  Downloading dulwich-0.20.26-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (546 kB)\n",
            "\u001b[K     |████████████████████████████████| 546 kB 47.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pydot>=1.2.4 in /usr/local/lib/python3.7/dist-packages (from dvc) (1.3.0)\n",
            "Collecting nanotime>=0.5.2\n",
            "  Downloading nanotime-0.5.2.tar.gz (3.2 kB)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.7/dist-packages (from dvc) (1.4.4)\n",
            "Collecting flufl.lock>=5\n",
            "  Downloading flufl.lock-6.0.tar.gz (30 kB)\n",
            "Requirement already satisfied: networkx>=2.5 in /usr/local/lib/python3.7/dist-packages (from dvc) (2.6.3)\n",
            "Requirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.7/dist-packages (from dvc) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from dvc) (3.10.0.2)\n",
            "Collecting pygtrie>=2.3.2\n",
            "  Downloading pygtrie-2.4.2.tar.gz (35 kB)\n",
            "Requirement already satisfied: tqdm<5,>=4.45.0 in /usr/local/lib/python3.7/dist-packages (from dvc) (4.62.3)\n",
            "Collecting flatten-dict<1,>=0.4.1\n",
            "  Downloading flatten_dict-0.4.2-py2.py3-none-any.whl (9.7 kB)\n",
            "Collecting gitpython>3\n",
            "  Downloading GitPython-3.1.24-py3-none-any.whl (180 kB)\n",
            "\u001b[K     |████████████████████████████████| 180 kB 48.5 MB/s \n",
            "\u001b[?25hCollecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.8-py3-none-any.whl (9.5 kB)\n",
            "Collecting diskcache>=5.2.1\n",
            "  Downloading diskcache-5.3.0-py3-none-any.whl (44 kB)\n",
            "\u001b[K     |████████████████████████████████| 44 kB 3.0 MB/s \n",
            "\u001b[?25hCollecting configobj>=5.0.6\n",
            "  Downloading configobj-5.0.6.tar.gz (33 kB)\n",
            "Requirement already satisfied: pyasn1>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from dvc) (0.4.8)\n",
            "Collecting pathspec<0.10.0,>=0.9.0\n",
            "  Downloading pathspec-0.9.0-py2.py3-none-any.whl (31 kB)\n",
            "Collecting ply>=3.9\n",
            "  Downloading ply-3.11-py2.py3-none-any.whl (49 kB)\n",
            "\u001b[K     |████████████████████████████████| 49 kB 5.8 MB/s \n",
            "\u001b[?25hCollecting dictdiffer>=0.8.1\n",
            "  Downloading dictdiffer-0.9.0-py2.py3-none-any.whl (16 kB)\n",
            "Collecting funcy>=1.14\n",
            "  Downloading funcy-1.16-py2.py3-none-any.whl (32 kB)\n",
            "Collecting ruamel.yaml>=0.17.11\n",
            "  Downloading ruamel.yaml-0.17.17-py3-none-any.whl (109 kB)\n",
            "\u001b[K     |████████████████████████████████| 109 kB 50.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata>=1.4 in /usr/local/lib/python3.7/dist-packages (from dvc) (4.8.2)\n",
            "Collecting shtab<2,>=1.3.4\n",
            "  Downloading shtab-1.5.2-py2.py3-none-any.whl (14 kB)\n",
            "Collecting aiohttp-retry>=2.4.5\n",
            "  Downloading aiohttp_retry-2.4.6-py3-none-any.whl (7.7 kB)\n",
            "Collecting pygit2>=1.5.0\n",
            "  Downloading pygit2-1.7.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.6 MB 49.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing>=2.4.7 in /usr/local/lib/python3.7/dist-packages (from dvc) (3.0.6)\n",
            "Collecting grandalf==0.6\n",
            "  Downloading grandalf-0.6-py3-none-any.whl (31 kB)\n",
            "Collecting psutil>=5.8.0\n",
            "  Downloading psutil-5.8.0-cp37-cp37m-manylinux2010_x86_64.whl (296 kB)\n",
            "\u001b[K     |████████████████████████████████| 296 kB 51.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tabulate>=0.8.7 in /usr/local/lib/python3.7/dist-packages (from dvc) (0.8.9)\n",
            "Collecting zc.lockfile>=1.2.1\n",
            "  Downloading zc.lockfile-2.0-py2.py3-none-any.whl (9.7 kB)\n",
            "Collecting fsspec[http]>=2021.10.1\n",
            "  Downloading fsspec-2021.11.1-py3-none-any.whl (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 51.6 MB/s \n",
            "\u001b[?25hCollecting colorama>=0.3.9\n",
            "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: importlib-resources>=5.2.2 in /usr/local/lib/python3.7/dist-packages (from dvc) (5.4.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from grandalf==0.6->dvc) (0.16.0)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 48.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from configobj>=5.0.6->dvc) (1.15.0)\n",
            "Requirement already satisfied: urllib3>=1.24.1 in /usr/local/lib/python3.7/dist-packages (from dulwich>=0.20.23->dvc) (1.24.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from dulwich>=0.20.23->dvc) (2021.10.8)\n",
            "Collecting atpublic\n",
            "  Downloading atpublic-2.3.tar.gz (16 kB)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.6 MB/s \n",
            "\u001b[?25hCollecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=1.4->dvc) (3.6.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from pygit2>=1.5.0->dvc) (1.5.2)\n",
            "Requirement already satisfied: cffi>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from pygit2>=1.5.0->dvc) (1.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.4.0->pygit2>=1.5.0->dvc) (2.21)\n",
            "Collecting ftfy\n",
            "  Downloading ftfy-6.0.3.tar.gz (64 kB)\n",
            "\u001b[K     |████████████████████████████████| 64 kB 2.6 MB/s \n",
            "\u001b[?25hCollecting phonenumbers\n",
            "  Downloading phonenumbers-8.12.38-py2.py3-none-any.whl (2.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.6 MB 39.0 MB/s \n",
            "\u001b[?25hCollecting xmltodict\n",
            "  Downloading xmltodict-0.12.0-py2.py3-none-any.whl (9.2 kB)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from python-benedict>=0.24.2->dvc) (3.13)\n",
            "Collecting mailchecker\n",
            "  Downloading mailchecker-4.1.1.tar.gz (222 kB)\n",
            "\u001b[K     |████████████████████████████████| 222 kB 37.6 MB/s \n",
            "\u001b[?25hCollecting python-fsutil\n",
            "  Downloading python_fsutil-0.5.0-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from python-benedict>=0.24.2->dvc) (2.8.2)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from python-benedict>=0.24.2->dvc) (5.0.2)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.22.0->dvc) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.22.0->dvc) (2.10)\n",
            "Collecting commonmark<0.10.0,>=0.9.0\n",
            "  Downloading commonmark-0.9.1-py2.py3-none-any.whl (51 kB)\n",
            "\u001b[K     |████████████████████████████████| 51 kB 6.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pygments<3.0.0,>=2.6.0 in /usr/local/lib/python3.7/dist-packages (from rich>=10.13.0->dvc) (2.6.1)\n",
            "Collecting ruamel.yaml.clib>=0.1.2\n",
            "  Downloading ruamel.yaml.clib-0.2.6-cp37-cp37m-manylinux1_x86_64.whl (546 kB)\n",
            "\u001b[K     |████████████████████████████████| 546 kB 47.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from zc.lockfile>=1.2.1->dvc) (57.4.0)\n",
            "Collecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
            "Collecting asynctest==0.13.0\n",
            "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 48.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->aiohttp-retry>=2.4.5->dvc) (2.0.8)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (192 kB)\n",
            "\u001b[K     |████████████████████████████████| 192 kB 50.5 MB/s \n",
            "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-5.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (160 kB)\n",
            "\u001b[K     |████████████████████████████████| 160 kB 35.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->aiohttp-retry>=2.4.5->dvc) (21.2.0)\n",
            "Collecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.1-py3-none-any.whl (5.7 kB)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from ftfy->python-benedict>=0.24.2->dvc) (0.2.5)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->python-benedict>=0.24.2->dvc) (1.3)\n",
            "Building wheels for collected packages: configobj, flufl.lock, nanotime, pygtrie, voluptuous, atpublic, ftfy, mailchecker\n",
            "  Building wheel for configobj (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for configobj: filename=configobj-5.0.6-py3-none-any.whl size=34546 sha256=2ebffdaf50b059077b9947034807b93984fff86b790c1dbf1bf7c234d22cc77e\n",
            "  Stored in directory: /root/.cache/pip/wheels/0d/c4/19/13d74440f2a571841db6b6e0a273694327498884dafb9cf978\n",
            "  Building wheel for flufl.lock (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for flufl.lock: filename=flufl.lock-6.0-py3-none-any.whl size=11995 sha256=0adabfbabba1f2347edfeed684f07e351c05e70e0d8d983e0b939f6aa9b8a90e\n",
            "  Stored in directory: /root/.cache/pip/wheels/d0/00/5c/f761a3cd38e03e313676e8c6bff1743d3dc280d58ab3f16ecd\n",
            "  Building wheel for nanotime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nanotime: filename=nanotime-0.5.2-py3-none-any.whl size=2440 sha256=cac44d7bc456a0dc6095f5669b04f71cf899e2c9f68ea9604ef757c20e0d5fb4\n",
            "  Stored in directory: /root/.cache/pip/wheels/b8/92/aa/456d462c908b4e210c3928f778d28f94049fc9e47af8b191c9\n",
            "  Building wheel for pygtrie (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pygtrie: filename=pygtrie-2.4.2-py3-none-any.whl size=19062 sha256=977ef90aa251be2b24f4bca185bc39f16b865618a806302c0bcb3326a3d80f9f\n",
            "  Stored in directory: /root/.cache/pip/wheels/d3/f8/ba/1d828b1603ea422686eb694253a43cb3a5901ea4696c1e0603\n",
            "  Building wheel for voluptuous (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for voluptuous: filename=voluptuous-0.12.2-py3-none-any.whl size=29562 sha256=941ce45325b35973aa7959eca957c7ed57d83075da19271d312570b0cbe52ff8\n",
            "  Stored in directory: /root/.cache/pip/wheels/ce/40/e9/5aba7699054584e118b04cc18d4d8f1f15f27af4a0d65ef4b4\n",
            "  Building wheel for atpublic (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for atpublic: filename=atpublic-2.3-py3-none-any.whl size=5033 sha256=57df01d102ce4e3d8f55da841d1de170b9caa91f87baa7ea1aa9674356140c24\n",
            "  Stored in directory: /root/.cache/pip/wheels/46/d9/0f/54be2ecb4bcb1612f987f0b6482d88fa7f3e43d3946f36a32a\n",
            "  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ftfy: filename=ftfy-6.0.3-py3-none-any.whl size=41933 sha256=527f9c0c4e676c19a8a324c582ebe5b86871a9bb4da7c85387989f3062e10fa2\n",
            "  Stored in directory: /root/.cache/pip/wheels/19/f5/38/273eb3b5e76dfd850619312f693716ac4518b498f5ffb6f56d\n",
            "  Building wheel for mailchecker (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mailchecker: filename=mailchecker-4.1.1-py3-none-any.whl size=222870 sha256=7571913d72b3d699f9b6ca1d9c84a8b728ba3c6a38535e7e9d06c91895ff9ea9\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/b7/c8/9c752c0de8b761960d6591758d7569ff8d4ce767a947c5ddf3\n",
            "Successfully built configobj flufl.lock nanotime pygtrie voluptuous atpublic ftfy mailchecker\n",
            "Installing collected packages: multidict, frozenlist, yarl, smmap, asynctest, async-timeout, aiosignal, xmltodict, ruamel.yaml.clib, python-fsutil, psutil, phonenumbers, mailchecker, gitdb, ftfy, fsspec, commonmark, colorama, atpublic, aiohttp, zc.lockfile, voluptuous, shtab, shortuuid, ruamel.yaml, rich, python-benedict, pygtrie, pygit2, ply, pathspec, nanotime, grandalf, gitpython, funcy, flufl.lock, flatten-dict, dulwich, dpath, distro, diskcache, dictdiffer, configobj, aiohttp-retry, dvc\n",
            "  Attempting uninstall: psutil\n",
            "    Found existing installation: psutil 5.4.8\n",
            "    Uninstalling psutil-5.4.8:\n",
            "      Successfully uninstalled psutil-5.4.8\n",
            "Successfully installed aiohttp-3.8.1 aiohttp-retry-2.4.6 aiosignal-1.2.0 async-timeout-4.0.1 asynctest-0.13.0 atpublic-2.3 colorama-0.4.4 commonmark-0.9.1 configobj-5.0.6 dictdiffer-0.9.0 diskcache-5.3.0 distro-1.6.0 dpath-2.0.5 dulwich-0.20.26 dvc-2.8.3 flatten-dict-0.4.2 flufl.lock-6.0 frozenlist-1.2.0 fsspec-2021.11.1 ftfy-6.0.3 funcy-1.16 gitdb-4.0.9 gitpython-3.1.24 grandalf-0.6 mailchecker-4.1.1 multidict-5.2.0 nanotime-0.5.2 pathspec-0.9.0 phonenumbers-8.12.38 ply-3.11 psutil-5.8.0 pygit2-1.7.1 pygtrie-2.4.2 python-benedict-0.24.3 python-fsutil-0.5.0 rich-10.15.2 ruamel.yaml-0.17.17 ruamel.yaml.clib-0.2.6 shortuuid-1.0.8 shtab-1.5.2 smmap-5.0.0 voluptuous-0.12.2 xmltodict-0.12.0 yarl-1.7.2 zc.lockfile-2.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "psutil"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1RIXO06f-bxS",
        "outputId": "a347634e-1c00-4c65-8897-29ba0e3c4332"
      },
      "source": [
        "!dvc get https://github.com/iterative/aita_dataset aita_clean.csv"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9vVXzcRv-kcq"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "df = pd.read_csv('aita_clean.csv')\n",
        "df = df[df['score'] >= 10]\n",
        "df['text'] = df['title'] + df['body'].fillna('')\n",
        "lines = df['text']\n",
        "labels = df['is_asshole']\n",
        "new_df = df[['text', 'is_asshole']]\n",
        "new_df.reset_index(drop=True, inplace=True) \n",
        "train_set, test_set = train_test_split(new_df, test_size=0.3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfw21JQU-nMt"
      },
      "source": [
        "from torchtext.data.utils import get_tokenizer\n",
        "from collections import Counter, OrderedDict\n",
        "from torchtext.vocab import vocab\n",
        "\n",
        "EMBEDDING_DIM=50\n",
        "VOCAB_SIZE=20000\n",
        "\n",
        "# Load English tokenizer, tagger, parser and NER\n",
        "tokenizer = get_tokenizer('spacy', language='en')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tq_OwOxh-q7b"
      },
      "source": [
        "# build the vocab\n",
        "counter = Counter()\n",
        "for i, line in enumerate(lines):\n",
        "    counter.update(tokenizer(str(line)))\n",
        "\n",
        "ordered_dict = OrderedDict(counter.most_common()[:VOCAB_SIZE])\n",
        "vocab = vocab(ordered_dict)\n",
        "\n",
        "# insert special tokens and set default index to 'unknown'\n",
        "vocab.insert_token('<PAD>', 0)\n",
        "vocab.insert_token('<UNK>', 1)\n",
        "vocab.set_default_index(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6oD0PGeP-2OQ"
      },
      "source": [
        "## Create embedding vectors from GloVe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5nolWE2o-4U4",
        "outputId": "b9c85af2-9ff2-4449-8c22-f64c1859e025"
      },
      "source": [
        "import torchtext as text\n",
        "\n",
        "# load glove embeddings\n",
        "vec = text.vocab.GloVe(name='6B', dim=50)\n",
        "# create the embedding matrix, a torch tensor in the shape (num_words+1, embedding_dim)\n",
        "word_emb = vec.get_vecs_by_tokens(vocab.get_itos())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ".vector_cache/glove.6B.zip: 862MB [02:43, 5.29MB/s]                           \n",
            "100%|█████████▉| 399999/400000 [00:13<00:00, 29890.24it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GhoaqwH_DQ-"
      },
      "source": [
        "## Build up train/test dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4GXXk_3_BXr"
      },
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torchtext.data.functional import to_map_style_dataset\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# transform input text to vectors\n",
        "def process_text(text):\n",
        "    return vocab(tokenizer(text))\n",
        "\n",
        "def collate_batch(batch):\n",
        "    label_list, text_list, lengths = [], [], []\n",
        "    for (_index, _text, _label) in batch:\n",
        "        label_list.append(_label)\n",
        "        processed_text = torch.tensor(process_text(_text), dtype=torch.int64)\n",
        "        text_list.append(processed_text)\n",
        "        lengths.append(processed_text.size(0))\n",
        "    lengths = torch.tensor(lengths, dtype=torch.float)\n",
        "\n",
        "    label_list = torch.tensor(label_list, dtype=torch.float)[:,None]\n",
        "\n",
        "    text_list = pad_sequence(text_list, batch_first=True)\n",
        "    return  label_list, text_list, lengths.to(device)\n",
        "\n",
        "train_dataset = to_map_style_dataset(train_set.itertuples())\n",
        "test_dataset = to_map_style_dataset(test_set.itertuples())\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=128,\n",
        "                              shuffle=True, collate_fn=collate_batch)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=128,\n",
        "                              shuffle=True, collate_fn=collate_batch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E1Frqn-uAAwx"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class LSTMcustom(nn.Module):\n",
        "    def __init__(self, word_vec, embed_dim):\n",
        "        super().__init__()\n",
        "        # embeddingbag outputs the average of all the words in a sentence\n",
        "        self.embedding = nn.Embedding(*(word_vec.size())).from_pretrained(word_vec, freeze=False)\n",
        "        # Create a 1D-CNN use torch.nn.Conv1d and feed the output to the next LSTM layer. \n",
        "        self.cnn = torch.nn.Conv1d(in_channels=embed_dim, out_channels=20, kernel_size=2)\n",
        "        # Determine the input shape of this LSTM layer.\n",
        "        self.lstm = nn.LSTM(20, 200, 1, bidirectional=False, batch_first = True)\n",
        "\n",
        "        self.fc = nn.Linear(200, 1)\n",
        "                \n",
        "    def forward(self, text, lengths):\n",
        "        embedded = self.embedding(text) # (batch_size, sent_len, emb_size)\n",
        "        # Original sequence and embedding_dim can change after applying CNN, use torch.permute to transpose\n",
        "\n",
        "        embedded = torch.permute(embedded, (0, 2, 1)) # (batch_size, emb_size, sent_len)\n",
        "        \n",
        "        cnn_out = self.cnn(embedded) # (batch_size, emb_size, sent_len)\n",
        "\n",
        "        cnn_out = torch.permute(cnn_out, (0, 2, 1)) # (batch_size, sent_len, emb_size)\n",
        "\n",
        "        lstm_out,_ = self.lstm(cnn_out) # lstm_out is a 3d tensor (batch_size, seq_len, output_size)\n",
        "        \n",
        "        lstm_out = lstm_out[:, -1, :]\n",
        "\n",
        "        return torch.sigmoid(self.fc(lstm_out)) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3swnoiTP_iqA"
      },
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "import numpy as np\n",
        "\n",
        "def train(dataloader):\n",
        "    model.train()\n",
        "    total_acc, total_count = 0, 0\n",
        "    log_interval = 50\n",
        "\n",
        "    for idx, (label, text, length) in enumerate(dataloader):\n",
        "        smt = SMOTE(random_state=42)\n",
        "        X_train, Y_train = smt.fit_resample(text, label)\n",
        "        lengths = []\n",
        "        label_list = torch.tensor(Y_train, dtype=torch.float)[:,None]\n",
        "        label = label_list.to(device)\n",
        "        lengths = torch.tensor(lengths, dtype=torch.float)\n",
        "        text = text.to(device)\n",
        "        lengths = lengths.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        # forward propagation\n",
        "        predicted_label = model(text, lengths)\n",
        "        # calculate loss and backpropagate to model paramters\n",
        "        loss = criterion(predicted_label, label)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
        "        # update parameters by stepping the optimizer\n",
        "        optimizer.step()\n",
        "        total_acc += ((predicted_label > 0.5) == label).sum().item()\n",
        "        total_count += label.size(0)\n",
        "        if idx % log_interval == 0 and idx > 0:\n",
        "            print('| epoch {:3d} | {:5d}/{:5d} batches '\n",
        "                  '| accuracy {:8.3f}'.format(epoch, idx, len(dataloader),\n",
        "                                              total_acc/total_count))\n",
        "            total_acc, total_count = 0, 0\n",
        "\n",
        "def evaluate(dataloader):\n",
        "    model.eval()\n",
        "    total_acc, total_count = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idx, (label, text, lengths) in enumerate(dataloader):\n",
        "            label = label.to(device)\n",
        "            text = text.to(device)\n",
        "            predicted_label = model(text, lengths)\n",
        "            loss = criterion(predicted_label, label)\n",
        "            total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
        "            total_count += label.size(0)\n",
        "    return total_acc/total_count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "As_Zg1vYGCRX"
      },
      "source": [
        "## Model Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23OkyNDt_kse",
        "outputId": "dbef1a8d-6af1-4c9b-b812-db124798fc87"
      },
      "source": [
        "EPOCHS = 1 # epoch\n",
        "\n",
        "model = LSTMcustom(word_vec=word_emb, embed_dim=EMBEDDING_DIM).to(device)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
        "criterion = torch.nn.BCELoss()\n",
        "total_accu = None\n",
        "\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    train(train_dataloader)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   1 |    50/  268 batches | accuracy    0.485\n",
            "| epoch   1 |   100/  268 batches | accuracy    0.620\n",
            "| epoch   1 |   150/  268 batches | accuracy    0.669\n",
            "| epoch   1 |   200/  268 batches | accuracy    0.702\n",
            "| epoch   1 |   250/  268 batches | accuracy    0.724\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6B-VL9UCGFuO"
      },
      "source": [
        "## Model saving and testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zff1FMjQ_2Ev",
        "outputId": "6e30ff26-ed53-4362-967e-7bc80d736a07"
      },
      "source": [
        "# Print model's state_dict\n",
        "print(\"Model's state_dict:\")\n",
        "for param_tensor in model.state_dict():\n",
        "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n",
        "\n",
        "#save the parameters of a model\n",
        "torch.save(model.state_dict(), 'CNN_NTA_model.pt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model's state_dict:\n",
            "embedding.weight \t torch.Size([20002, 50])\n",
            "cnn.weight \t torch.Size([20, 50, 2])\n",
            "cnn.bias \t torch.Size([20])\n",
            "lstm.weight_ih_l0 \t torch.Size([800, 20])\n",
            "lstm.weight_hh_l0 \t torch.Size([800, 200])\n",
            "lstm.bias_ih_l0 \t torch.Size([800])\n",
            "lstm.bias_hh_l0 \t torch.Size([800])\n",
            "fc.weight \t torch.Size([1, 200])\n",
            "fc.bias \t torch.Size([1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZFEtkbQgF4Ob",
        "outputId": "685beb14-f930-4038-e4d4-aad93f4cbb52"
      },
      "source": [
        "accu_test = evaluate(test_dataloader)\n",
        "print('test accuracy {:8.2f}%'.format(accu_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test accuracy    91.58%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jw-Z6hhGW-i"
      },
      "source": [
        "def predict(sentence, model):\n",
        "    model.eval()\n",
        "    text_list = []\n",
        "    lengths = []\n",
        "    processed_text = torch.tensor(process_text(sentence), dtype=torch.int64)\n",
        "    text_list.append(processed_text)\n",
        "    lengths.append(processed_text.size(0))\n",
        "    text_list = pad_sequence(text_list, batch_first=True)\n",
        "    lengths = torch.tensor(lengths, dtype=torch.float)\n",
        "    with torch.no_grad():\n",
        "        predicted_label = model(text_list.to(device), lengths.to(device))\n",
        "    label = (predicted_label.cpu().numpy()[0] > 0.5)\n",
        "    if label == 0:\n",
        "        print('The predict label is : 0, and the class is no asshole with the probability of {}.'.format(1 - predicted_label.item()))\n",
        "    else:\n",
        "        print('The predict label is : 1, and the class is asshole with the probability of {}.'.format(predicted_label.item()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGc_exr8WPyD"
      },
      "source": [
        "def acc_for_each_class(model,dataloader):\n",
        "    model.eval()\n",
        "    normal_acc, normal_count = 0 , 0\n",
        "    pneumonia_acc, pneumonia_count = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idx, (label, text, lengths) in enumerate(dataloader):\n",
        "            label = label.to(device)\n",
        "            text = text.to(device)\n",
        "            predicted_label = model(text, lengths)\n",
        "            correct=((predicted_label > 0.5) == label)\n",
        "\n",
        "            for i,l in enumerate(label):\n",
        "                    if(l.data[0] < 0.5):\n",
        "                        normal_count += 1\n",
        "                        if(correct[i].data[0]):\n",
        "                            normal_acc += 1\n",
        "                    else:\n",
        "                        pneumonia_count += 1.0\n",
        "                        if(correct[i].data[0]):\n",
        "                            pneumonia_acc += 1\n",
        "      \n",
        "    return normal_acc/normal_count, pneumonia_acc/pneumonia_count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94rjaEX6WQq9",
        "outputId": "2dc374d6-fcb9-4eb2-f506-2bd5eefff081"
      },
      "source": [
        "nonasshole_acc, asshole_acc = acc_for_each_class(model, test_dataloader)\n",
        "print('The accuracy for the non-asshole calss is : {}'.format(nonasshole_acc))\n",
        "print('The accuracy for the asshole calss is : {}'.format(asshole_acc))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The accuracy for the non-asshole calss is : 0.9875344942430297\n",
            "The accuracy for the asshole calss is : 0.011815770436460092\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8g7ivsDSe3s"
      },
      "source": [
        "def get_f1_score(model,dataloader,class_name):\n",
        "    model.eval()\n",
        "    FP = 0\n",
        "    TP = 0\n",
        "    FN = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idx, (label, text, lengths) in enumerate(dataloader):\n",
        "            label = label.to(device)\n",
        "            text = text.to(device)\n",
        "            predicted_label = model(text, lengths)\n",
        "            correct=((predicted_label > 0.5) == label)\n",
        "\n",
        "            for i,l in enumerate(label):\n",
        "                if class_name == 'asshole':\n",
        "                  if(l.data[0] > 0.5):\n",
        "                      if(correct[i].data[0]):\n",
        "                          TP += 1\n",
        "                      else:\n",
        "                          FP += 1\n",
        "                  else:\n",
        "                      if(not correct[i].data[0]):\n",
        "                          FN += 1\n",
        "                else:\n",
        "                      if(l.data[0] < 0.5):\n",
        "                          if(correct[i].data[0]):\n",
        "                              TP += 1\n",
        "                          else:\n",
        "                              FP += 1\n",
        "                      else:\n",
        "                          if(not correct[i].data[0]):\n",
        "                              FN += 1\n",
        "      \n",
        "    return TP/(TP + 0.5 * (FP + FN))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KVb4QOv_SgSv",
        "outputId": "873cfb44-f85e-44f0-803f-ea45a4483b4f"
      },
      "source": [
        "f1_score_asshole = get_f1_score(model, test_dataloader, 'asshole')\n",
        "print('The f1 score for asshole class is : {}'.format(f1_score_asshole))\n",
        "f1_score_nonasshole = get_f1_score(model, test_dataloader, 'nonasshole')\n",
        "print('The f1 score for nonasshole class is : {}'.format(f1_score_nonasshole))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The f1 score for asshole class is : 0.026715799170888992\n",
            "The f1 score for nonasshole class is : 0.8300965042245625\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jimFt6UnGLpX",
        "outputId": "4e5ba031-c3e7-4490-e143-33127b1090c8"
      },
      "source": [
        "sentence = test['text'].iloc[0]\n",
        "predict(sentence, model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The predict label is : 0, and the class is no asshole with the probability of 0.6079334020614624.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKGU7oJEHg0M"
      },
      "source": [
        "## Model Loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HNqrBgUFHlzA",
        "outputId": "6539fcd3-de13-4d48-949f-bbae767f23d7"
      },
      "source": [
        "model = LSTMcustom(word_vec=word_emb, embed_dim=EMBEDDING_DIM).to(device)\n",
        "model.load_state_dict(torch.load('CNN_NTA_model.pt'))\n",
        "model.eval()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LSTMcustom(\n",
              "  (embedding): Embedding(20002, 50)\n",
              "  (cnn): Conv1d(50, 20, kernel_size=(2,), stride=(1,))\n",
              "  (lstm): LSTM(20, 200, batch_first=True)\n",
              "  (fc): Linear(in_features=200, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EvCBeFrAtZS_",
        "outputId": "f4cd9304-8928-47a0-b5aa-822cb0306001"
      },
      "source": [
        "sentence = test['text'].iloc[50]\n",
        "predict(sentence, model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The predict label is : 0, and the class is no asshole with the probability of 0.6067976653575897.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Wyc-KiEIRGY",
        "outputId": "66b532f1-c317-483e-de4c-c4a50ebbf8bb"
      },
      "source": [
        "sentence = '''It seems like I came off as an asshole. What can I do in the future to avoid this? I thought I was just randomly explaining something, which seems common on Reddit.\n",
        "\n",
        "I definitely should not have said \"Sorry for trying to help\" - That was pretty immature and rude. \n",
        "\n",
        "Am I the asshole here?\n",
        "\n",
        "I wrote as little as possible to skew perspective as little as possible.'''\n",
        "predict(sentence, model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The predict label is : 0, and the class is no asshole with the probability of 0.6045775413513184.\n"
          ]
        }
      ]
    }
  ]
}